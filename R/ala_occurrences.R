#' Occurrence records
#'
#' Large downloads can take a long time to process. To show a progress bar
#' for the download, set \code{verbose = TRUE} in \code{\link{ala_config}}. The
#' maximum number of records that can be downloaded in one go is 50 million.
#'
#' @param taxa data.frame: generated by a call to \code{\link{select_taxa}}. This
#' argument also accepts a vector of unique species identifiers.
#' @param filters data.frame: generated by a call to \code{\link{select_filters}}
#' @param locations string: generated by a call to \code{\link{select_locations}}
#' @param columns data.frame: generated by a call to \code{\link{select_columns}}
#' @param mint_doi logical: by default no DOI will be generated. Set to
#' true if you intend to use the data in a publication or similar
#' @return data.frame of occurrences
#' @examples \dontrun{
#' # Search for occurrences matching a taxon identifier
#' occ <- ala_occurrences(taxa = select_taxa("Reptilia"))
#'
#' # Search for occurrences in a year range
#' occ <- ala_occurrences(filters = select_filters(year = seq(2010, 2020)))
#'
#' # Search for occurrences in a WKT-specified area
#' polygon <- "POLYGON((146.24960 -34.05930,146.37045 -34.05930,146.37045 -34.152549,146.24960 -34.15254,146.24960 -34.05930))"
#' occ <- ala_occurrences(locations = select_locations(wkt = polygon))
#' }
#' @export ala_occurrences

ala_occurrences <- function(taxa, filters, locations, columns,
                            mint_doi = FALSE) {

  config_verbose <- getOption("galah_config")$verbose
  assert_that(is.logical(mint_doi))
  assert_that(is.logical(config_verbose))
  query <- list()

  if (missing(taxa) & missing(filters) & missing(locations)) {
    # stop or allow users to download the whole ALA?
    stop("Need to provide one of `taxon id`, `filters` or `area`")
  }

  if (!missing(taxa)) {
    # should species id be validated?
    if (inherits(taxa, "data.frame") &&
        "taxon_concept_id" %in% colnames(taxa)) {
      taxa <- taxa$taxon_concept_id
    }
    assert_that(is.character(taxa))
    taxa_query <- build_taxa_query(taxa)
  } else {
    taxa_query <- NULL
  }

  # validate filters
  if (!missing(filters)) {
    assert_that(is.data.frame(filters))
    filter_query <- build_filter_query(filters)
  } else {
    filter_query <- NULL
  }

  query$fq <- c(taxa_query, filter_query)

  if (!missing(locations)) {
    area_query <- locations
    query$wkt <- area_query
  } else {
    area_query <- NULL
  }

  # Add columns after getting record count
  if(missing(columns)) {
    if (config_verbose) {
      message("No columns specified, default columns will be returned.")
      }
    columns <- select_columns(group = "basic")
  }
  # handle caching
  # look for file to download- if it doesn't exist, continue on
  # use base_url_biocache as the filename? otherwise won't be found
  # create cache file with the original query
  caching <- getOption("galah_config")$caching

  if (caching) {
    cache_file <- cache_filename(c(getOption("galah_server_config")$
                                     base_url_biocache,
                                   path = "ws/occurrences/offline/download",
                                   params = unlist(query)), ext = ".zip")
    if (file.exists(cache_file)) {
      if (config_verbose) { message("Using existing file") }

      # look for file using query parameters
      data <- read.csv(unz(cache_file, "data.csv"), stringsAsFactors = FALSE)
      # if file doesn't exist, continue as before
      return(data)
    }
  } else {
    cache_file <- tempfile(fileext = ".zip")
  }

  if (check_for_caching(taxa_query, filter_query, area_query, columns)) {
    query <- cached_query(taxa_query, filter_query, area_query)
  }

  count <- record_count(query)
  check_count(count, config_verbose)

  assertion_cols <- columns[columns$type == "assertions", ]
  query$fields <- build_columns(columns[columns$type != "assertions", ])
  query$qa <- build_columns(assertion_cols)

  if (mint_doi) {
    query$mintDoi <- "true"
  }
  query$mintDoi <- mint_doi
  query$emailNotify <- email_notify()

  # Get data
  url <- getOption("galah_server_config")$base_url_biocache
  query <- c(query, email = user_email(), reasonTypeId = download_reason(),
             dwcHeaders = "true")

  download_path <- wait_for_download(url, query, config_verbose)
  data_path <- ala_download(url = url,
                       path = download_path,
                       cache_file = cache_file, ext = ".zip")

  #TODO: safely read csv
  df <- read.csv(unz(data_path, "data.csv"), stringsAsFactors = FALSE)

  # rename cols so they match requested cols
  names(df) <- rename_columns(names(df), type = "occurrence")

  # replace 'true' and 'false' with boolean
  if (nrow(assertion_cols) > 0) {
    df <- fix_assertion_cols(df, assertion_cols$name)
  }

  # add DOI as attribute
  doi <- NA
  if (as.logical(mint_doi)) {
    tryCatch(
      doi <- as.character(
        read.table(unz(data_path, "doi.txt"))$V1),
      warning = function(e) {
        e$message <- "No DOI was generated for download. The DOI server may
                        be down. Please try again later"
      })
  }
  attr(df, "doi") <- doi

  return(df)
}



wait_for_download <- function(url, query, verbose) {
  status <- ala_GET(url, "ws/occurrences/offline/download",
                    params = query, on_error = occ_error_handler)

  status_url <- parse_url(status$statusUrl)
  status <- ala_GET(url, path = status_url$path)

  # create a progress bar
  if (verbose) {
    pb <- txtProgressBar(max = 1, style = 3)
  }

  while(status$status == "inQueue") {
    status <- ala_GET(url, path = status_url$path)
  }

  while (tolower(status$status) == "running") {
    val <- (status$records / status$totalRecords)
    if (verbose) {
      setTxtProgressBar(pb, val)
    }
    status <- ala_GET(url, path = status_url$path)
    Sys.sleep(2)
  }
  if (verbose) {
    setTxtProgressBar(pb, value = 1)
    close(pb)
  }
  parse_url(status$downloadUrl)$path
}

check_count <- function(count, config_verbose) {
  if (count == 0) {
    stop("This query does not match any records.")
  } else if (count > 50000000) {
    stop("A maximum of 50 million records can be retrieved at once.",
         " Please narrow the query and try again.")
  } else {
    if (config_verbose) { message("This query will return ", count, " records") }
  }
}

download_reason <- function() {
  reason <- getOption("galah_config")$download_reason_id
  if (reason == "") {
    reason <- 4
  }
  reason
}


email_notify <- function() {
  notify <- as.logical(getOption("galah_config")$send_email)
  if (is.na(notify)) {
    notify <- FALSE
  }
  # ala api requires lowercase
  ifelse(notify, "true", "false")
}

user_email <- function() {
  email <- getOption("galah_config")$email
  if (email == "") {
    email <- Sys.getenv("email")
  }
  if (email == "") {
    stop("To download occurrence records you must provide a valid email ",
         "address registered with the ALA using `ala_config(email = )`")
  }
  email
}

occ_error_handler <- function(code) {
  if (code == 403) {
    stop("Status code 403 was returned for this occurrence download request. This may be because
  the email you provided is not registered with the ALA. Please check and try again.")
  }
}
